---
sidebar_position: 3
title: 'How Python AI Agents Receive Sensor Data from ROS 2'
keywords: [ros2, python, ai, agent, sensor, data, integration, robotics, perception]
description: 'Detailed explanation of how AI agents written in Python receive sensor data from ROS 2 controllers and other nodes'
---

# How Python AI Agents Receive Sensor Data from ROS 2

## Learning Objectives

After completing this section, you will be able to:
- Understand the different methods for receiving sensor data in AI agents from ROS 2
- Implement sensor data subscription using ROS 2 topics
- Use ROS 2 services for on-demand sensor data requests
- Apply ROS 2 actions for complex sensor data retrieval tasks
- Design appropriate sensor interfaces for different types of AI processing
- Process and interpret sensor data for AI decision-making

## Overview of Sensor Data Communication Patterns

Python AI agents can receive sensor data from ROS 2 through several communication patterns, each suited for different types of sensor information:

### 1. Topic-Based Sensor Data Subscription
- **Use case**: Continuous sensor streams (camera images, LIDAR scans, IMU data)
- **Pattern**: Publisher-Subscriber
- **Characteristics**: Asynchronous, continuous data flow
- **Example**: Receiving camera images for computer vision processing

### 2. Service-Based Sensor Data Requests
- **Use case**: On-demand sensor data retrieval (current pose, specific sensor readings)
- **Pattern**: Client-Server Request/Response
- **Characteristics**: Synchronous, with immediate response
- **Example**: Requesting current robot pose for planning

### 3. Action-Based Sensor Data Retrieval
- **Use case**: Complex sensor tasks with feedback (3D mapping, sensor calibration)
- **Pattern**: Client-Server with feedback loop
- **Characteristics**: Asynchronous with continuous feedback
- **Example**: Requesting a full 3D scan of the environment

## Topic-Based Sensor Data Subscription

### Concept
The AI agent subscribes to sensor topics published by various ROS 2 nodes. This pattern is ideal for continuous sensor streams that the AI agent needs to process in real-time.

### Implementation Example

```python
#!/usr/bin/env python3
"""
AI Agent Sensor Subscriber

This node represents an AI agent that subscribes to sensor data from ROS 2.
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, LaserScan, Imu, JointState
from nav_msgs.msg import Odometry
from geometry_msgs.msg import Twist
from std_msgs.msg import Float64MultiArray
import numpy as np
from cv_bridge import CvBridge
import cv2


class AISensorSubscriber(Node):
    """
    An AI agent that subscribes to sensor data from ROS 2 nodes.
    """

    def __init__(self):
        super().__init__('ai_sensor_subscriber')

        # Initialize CV bridge for image processing
        self.cv_bridge = CvBridge()

        # Subscribers for different sensor types
        self.image_subscriber = self.create_subscription(
            Image, '/camera/image_raw', self.image_callback, 10)

        self.laser_subscriber = self.create_subscription(
            LaserScan, '/scan', self.laser_callback, 10)

        self.imu_subscriber = self.create_subscription(
            Imu, '/imu/data', self.imu_callback, 10)

        self.odom_subscriber = self.create_subscription(
            Odometry, '/odom', self.odom_callback, 10)

        self.joint_state_subscriber = self.create_subscription(
            JointState, '/joint_states', self.joint_state_callback, 10)

        self.force_torque_subscriber = self.create_subscription(
            Float64MultiArray, '/force_torque', self.force_torque_callback, 10)

        # Store latest sensor data
        self.latest_image = None
        self.latest_laser = None
        self.latest_imu = None
        self.latest_odom = None
        self.latest_joints = None
        self.latest_force_torque = None

        # AI processing state
        self.ai_state = "IDLE"
        self.ai_processing_frequency = 5.0  # Hz
        self.ai_timer = self.create_timer(1.0/self.ai_processing_frequency, self.ai_process)

        self.get_logger().info('AI Sensor Subscriber initialized')

    def image_callback(self, msg):
        """
        Handle incoming camera image data.
        """
        try:
            # Convert ROS Image message to OpenCV image
            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

            # Store latest image
            self.latest_image = cv_image

            # Log information about the image
            self.get_logger().info(f'Image received: {cv_image.shape[1]}x{cv_image.shape[0]}')

            # Process image for AI (example: simple object detection placeholder)
            self.process_image_for_ai(cv_image)

        except Exception as e:
            self.get_logger().error(f'Error processing image: {e}')

    def laser_callback(self, msg):
        """
        Handle incoming LIDAR scan data.
        """
        # Store latest laser scan
        self.latest_laser = msg

        # Extract relevant information
        ranges = np.array(msg.ranges)
        # Filter out invalid ranges (inf, nan)
        valid_ranges = ranges[np.isfinite(ranges)]

        if len(valid_ranges) > 0:
            min_distance = np.min(valid_ranges)
            max_distance = np.max(valid_ranges)
            avg_distance = np.mean(valid_ranges)

            self.get_logger().info(f'Laser scan: min={min_distance:.2f}, avg={avg_distance:.2f}, max={max_distance:.2f}')

            # Process laser data for AI
            self.process_laser_for_ai(msg)

    def imu_callback(self, msg):
        """
        Handle incoming IMU data.
        """
        # Store latest IMU data
        self.latest_imu = msg

        # Extract orientation and angular velocity
        orientation = msg.orientation
        angular_velocity = msg.angular_velocity
        linear_acceleration = msg.linear_acceleration

        self.get_logger().info(f'IMU: orientation=({orientation.x:.3f}, {orientation.y:.3f}, {orientation.z:.3f}, {orientation.w:.3f})')

        # Process IMU data for AI
        self.process_imu_for_ai(msg)

    def odom_callback(self, msg):
        """
        Handle incoming odometry data.
        """
        # Store latest odometry
        self.latest_odom = msg

        # Extract position and velocity
        position = msg.pose.pose.position
        velocity = msg.twist.twist.linear

        self.get_logger().info(f'Odom: pos=({position.x:.2f}, {position.y:.2f}, {position.z:.2f}), vel=({velocity.x:.2f}, {velocity.y:.2f}, {velocity.z:.2f})')

        # Process odometry for AI
        self.process_odom_for_ai(msg)

    def joint_state_callback(self, msg):
        """
        Handle incoming joint state data.
        """
        # Store latest joint states
        self.latest_joints = msg

        # Log joint positions
        joint_info = []
        for i, name in enumerate(msg.name):
            if i < len(msg.position):
                joint_info.append(f'{name}: {msg.position[i]:.3f}')

        if joint_info:
            self.get_logger().info(f'Joint states: {", ".join(joint_info)}')

        # Process joint data for AI
        self.process_joint_for_ai(msg)

    def force_torque_callback(self, msg):
        """
        Handle incoming force/torque sensor data.
        """
        # Store latest force/torque data
        self.latest_force_torque = msg

        # Process force/torque data
        forces = list(msg.data)
        self.get_logger().info(f'Force/torque: {forces}')

        # Process force/torque for AI
        self.process_force_torque_for_ai(msg)

    def process_image_for_ai(self, image):
        """
        Process image data for AI decision making.
        """
        # Example: Simple object detection (placeholder)
        height, width, channels = image.shape

        # Example: Detect if there's something in the center of the image
        center_region = image[height//2-50:height//2+50, width//2-50:width//2+50]
        center_brightness = np.mean(center_region)

        if center_brightness > 150:  # Arbitrary threshold
            self.get_logger().info('Bright object detected in center')
        else:
            self.get_logger().info('No bright object in center')

    def process_laser_for_ai(self, laser_msg):
        """
        Process LIDAR data for AI decision making.
        """
        # Example: Check for obstacles in front
        ranges = np.array(laser_msg.ranges)
        valid_ranges = ranges[np.isfinite(ranges)]

        if len(valid_ranges) > 0:
            # Look at front 30 degrees (assuming 180-degree LIDAR)
            front_start = len(valid_ranges) // 2 - 15
            front_end = len(valid_ranges) // 2 + 15

            if 0 <= front_start < len(valid_ranges) and 0 <= front_end < len(valid_ranges):
                front_ranges = valid_ranges[front_start:front_end]

                if len(front_ranges) > 0:
                    min_front_dist = np.min(front_ranges)

                    if min_front_dist < 0.5:  # Less than 50cm
                        self.get_logger().warn('OBSTACLE DETECTED IN FRONT')
                        self.ai_state = "OBSTACLE_AVOIDANCE"
                    else:
                        self.ai_state = "NAVIGATION"

    def process_imu_for_ai(self, imu_msg):
        """
        Process IMU data for AI decision making.
        """
        # Example: Check if robot is tilted too much
        orientation = imu_msg.orientation
        # Convert quaternion to roll/pitch/yaw (simplified)
        # In practice, use tf_transformations or similar

        # Check if absolute pitch or roll is too high (indicating potential instability)
        # This is a simplified check
        if abs(orientation.x) > 0.5 or abs(orientation.y) > 0.5:
            self.get_logger().warn('Robot tilt exceeds safe limits')
            self.ai_state = "STABILIZATION"

    def process_odom_for_ai(self, odom_msg):
        """
        Process odometry data for AI decision making.
        """
        # Example: Check if robot is moving as expected
        position = odom_msg.pose.pose.position
        velocity = odom_msg.twist.twist.linear

        # Log current position for path planning
        self.get_logger().info(f'Robot at position: ({position.x:.2f}, {position.y:.2f})')

    def process_joint_for_ai(self, joint_msg):
        """
        Process joint state data for AI decision making.
        """
        # Example: Check if gripper is in expected state
        for i, name in enumerate(joint_msg.name):
            if 'gripper' in name.lower():
                if i < len(joint_msg.position):
                    gripper_pos = joint_msg.position[i]
                    if gripper_pos > 0.01:  # Gripper open
                        self.get_logger().info('Gripper is open')
                    else:  # Gripper closed
                        self.get_logger().info('Gripper is closed')

    def process_force_torque_for_ai(self, ft_msg):
        """
        Process force/torque data for AI decision making.
        """
        # Example: Check if gripper is holding something
        if len(ft_msg.data) >= 3:
            force_z = ft_msg.data[2]  # Assuming Z-axis is the gripping direction
            if abs(force_z) > 5.0:  # Arbitrary threshold
                self.get_logger().info('Object detected in gripper')
            else:
                self.get_logger().info('No object in gripper')

    def ai_process(self):
        """
        Main AI processing loop that uses all sensor data.
        """
        self.get_logger().info(f'AI Processing - State: {self.ai_state}')

        # Example: Simple decision making based on sensor fusion
        if self.ai_state == "OBSTACLE_AVOIDANCE":
            self.get_logger().info('Executing obstacle avoidance behavior')
        elif self.ai_state == "NAVIGATION":
            self.get_logger().info('Executing navigation behavior')
        elif self.ai_state == "STABILIZATION":
            self.get_logger().info('Executing stabilization behavior')
        else:
            self.get_logger().info('Executing default behavior')


def main(args=None):
    """
    Main function to run the AI sensor subscriber.
    """
    rclpy.init(args=args)
    ai_subscriber = AISensorSubscriber()

    try:
        rclpy.spin(ai_subscriber)
    except KeyboardInterrupt:
        pass
    finally:
        ai_subscriber.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Service-Based Sensor Data Requests

### Concept
The AI agent calls services to request specific sensor data on demand, which is useful for infrequently accessed information or when the AI needs specific data at specific times.

### Implementation Example

```python
#!/usr/bin/env python3
"""
AI Agent Sensor Service Client

This node represents an AI agent that requests sensor data via services.
"""

import rclpy
from rclpy.node import Node
from example_interfaces.srv import Trigger
from std_srvs.srv import Empty
from sensor_msgs.srv import SetCameraInfo
from geometry_msgs.srv import GetTransform


class AISensorServiceClient(Node):
    """
    An AI agent that requests sensor data via services.
    """

    def __init__(self):
        super().__init__('ai_sensor_service_client')

        # Client for requesting sensor calibration
        self.calibrate_sensor_client = self.create_client(Trigger, '/calibrate_sensors')

        # Client for requesting sensor reset
        self.reset_sensor_client = self.create_client(Empty, '/reset_sensors')

        # Wait for services to be available
        while not self.calibrate_sensor_client.wait_for_service(timeout_sec=1.0):
            self.get_logger().info('Calibration service not available, waiting...')

        while not self.reset_sensor_client.wait_for_service(timeout_sec=1.0):
            self.get_logger().info('Reset service not available, waiting...')

        # Timer to periodically request sensor operations
        self.timer = self.create_timer(30.0, self.request_sensor_operations)  # Every 30 seconds
        self.operation_count = 0

        self.get_logger().info('AI Sensor Service Client initialized')

    def request_sensor_operations(self):
        """
        Request sensor operations via services.
        """
        self.operation_count += 1

        if self.operation_count % 3 == 1:
            # Request sensor calibration
            self.request_sensor_calibration()
        elif self.operation_count % 3 == 2:
            # Request sensor reset
            self.request_sensor_reset()

    def request_sensor_calibration(self):
        """
        Request sensor calibration via service.
        """
        request = Trigger.Request()

        future = self.calibrate_sensor_client.call_async(request)
        future.add_done_callback(self.calibration_response_callback)

        self.get_logger().info('Sensor calibration requested')

    def request_sensor_reset(self):
        """
        Request sensor reset via service.
        """
        request = Empty.Request()

        future = self.reset_sensor_client.call_async(request)
        future.add_done_callback(self.reset_response_callback)

        self.get_logger().info('Sensor reset requested')

    def calibration_response_callback(self, future):
        """
        Handle calibration service response.
        """
        try:
            response = future.result()
            if response.success:
                self.get_logger().info(f'Calibration successful: {response.message}')
            else:
                self.get_logger().error(f'Calibration failed: {response.message}')
        except Exception as e:
            self.get_logger().error(f'Calibration service call failed: {e}')

    def reset_response_callback(self, future):
        """
        Handle reset service response.
        """
        try:
            future.result()
            self.get_logger().info('Sensor reset completed')
        except Exception as e:
            self.get_logger().error(f'Reset service call failed: {e}')


def main(args=None):
    """
    Main function to run the AI sensor service client.
    """
    rclpy.init(args=args)
    ai_client = AISensorServiceClient()

    try:
        rclpy.spin(ai_client)
    except KeyboardInterrupt:
        pass
    finally:
        ai_client.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Advanced Sensor Data Processing

### Sensor Data Filtering and Preprocessing

```python
import numpy as np
from scipy import ndimage
from collections import deque


class SensorDataProcessor:
    """
    Advanced sensor data processing utilities for AI agents.
    """

    def __init__(self, history_size=10):
        self.history_size = history_size
        self.laser_history = deque(maxlen=history_size)
        self.imu_history = deque(maxlen=history_size)

    def filter_laser_data(self, laser_msg):
        """
        Apply filtering to laser scan data.
        """
        # Convert to numpy array
        ranges = np.array(laser_msg.ranges)

        # Replace inf values with max_range
        ranges[np.isinf(ranges)] = laser_msg.range_max
        ranges[ranges > laser_msg.range_max] = laser_msg.range_max
        ranges[ranges < laser_msg.range_min] = laser_msg.range_min

        # Apply median filter to remove noise
        filtered_ranges = ndimage.median_filter(ranges, size=3)

        # Store in history for temporal filtering
        self.laser_history.append(filtered_ranges)

        return filtered_ranges

    def detect_laser_anomalies(self, laser_msg):
        """
        Detect anomalies in laser scan data (e.g., dynamic obstacles).
        """
        current_ranges = np.array(laser_msg.ranges)

        if len(self.laser_history) > 1:
            # Compare with previous scans to detect changes
            prev_ranges = np.array(self.laser_history[-2])

            # Calculate differences
            differences = np.abs(current_ranges - prev_ranges)

            # Find significant changes (potential dynamic obstacles)
            threshold = 0.1  # 10cm threshold
            dynamic_indices = np.where(differences > threshold)[0]

            if len(dynamic_indices) > 0:
                self.get_logger().info(f'Detected {len(dynamic_indices)} potential dynamic obstacles')

        return current_ranges

    def smooth_imu_data(self, imu_msg):
        """
        Apply smoothing to IMU data to reduce noise.
        """
        # Store in history
        self.imu_history.append(imu_msg)

        if len(self.imu_history) >= 3:
            # Simple moving average for orientation (using quaternions properly would require special handling)
            avg_orientation = np.mean([
                [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w]
                for msg in list(self.imu_history)[-3:]  # Last 3 readings
            ], axis=0)

            # Normalize quaternion
            norm = np.linalg.norm(avg_orientation)
            if norm > 0:
                avg_orientation = avg_orientation / norm

            # Create smoothed IMU message
            smoothed_imu = Imu()
            smoothed_imu.orientation.x = avg_orientation[0]
            smoothed_imu.orientation.y = avg_orientation[1]
            smoothed_imu.orientation.z = avg_orientation[2]
            smoothed_imu.orientation.w = avg_orientation[3]

            return smoothed_imu

        return imu_msg
```

### Multi-Sensor Fusion

```python
class MultiSensorFusion:
    """
    Fusion of data from multiple sensors for AI decision making.
    """

    def __init__(self):
        self.fused_position = None
        self.fused_orientation = None
        self.confidence_scores = {}

    def fuse_position_data(self, odom_pos, imu_pos, gps_pos=None):
        """
        Fuse position data from multiple sensors using weighted averaging.
        """
        # Example fusion algorithm (simplified)
        weights = {
            'odom': 0.6,  # Odometry is usually most reliable short-term
            'imu': 0.3,   # IMU provides good orientation data
            'gps': 0.1    # GPS provides absolute position but with lower frequency
        }

        if gps_pos is not None:
            # Weighted fusion
            fused_x = (weights['odom'] * odom_pos.x +
                      weights['imu'] * imu_pos.x +
                      weights['gps'] * gps_pos.x)
            fused_y = (weights['odom'] * odom_pos.y +
                      weights['imu'] * imu_pos.y +
                      weights['gps'] * gps_pos.y)
        else:
            # Without GPS
            fused_x = weights['odom'] * odom_pos.x + weights['imu'] * imu_pos.x
            fused_y = weights['odom'] * odom_pos.y + weights['imu'] * imu_pos.y

        self.fused_position = (fused_x, fused_y)
        return self.fused_position

    def detect_sensor_conflicts(self, sensor_data):
        """
        Detect conflicts between different sensor readings.
        """
        conflicts = []

        # Example: Check if LIDAR and camera data are consistent
        if 'lidar' in sensor_data and 'camera' in sensor_data:
            lidar_distance = sensor_data['lidar']['closest_obstacle']
            camera_distance = sensor_data['camera']['estimated_distance']

            if abs(lidar_distance - camera_distance) > 0.5:  # 50cm threshold
                conflicts.append({
                    'type': 'distance_estimation_conflict',
                    'sensors': ['lidar', 'camera'],
                    'difference': abs(lidar_distance - camera_distance)
                })

        return conflicts
```

## Sensor Data Quality Assessment

### Data Quality Metrics

```python
class SensorQualityAssessment:
    """
    Assess the quality of sensor data for AI processing.
    """

    def __init__(self):
        self.quality_thresholds = {
            'image_sharpness': 100,      # Laplacian variance threshold
            'laser_min_valid': 0.1,      # Minimum valid range
            'imu_variance': 0.01,        # Maximum acceptable variance
            'data_age': 1.0              # Maximum acceptable data age in seconds
        }

    def assess_image_quality(self, image):
        """
        Assess the quality of an image for AI processing.
        """
        # Convert to grayscale for sharpness assessment
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # Calculate Laplacian variance (measure of sharpness)
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()

        quality_metrics = {
            'sharpness': laplacian_var,
            'brightness': np.mean(gray),
            'contrast': np.std(gray),
            'is_usable': laplacian_var > self.quality_thresholds['image_sharpness']
        }

        return quality_metrics

    def assess_laser_quality(self, laser_msg):
        """
        Assess the quality of LIDAR data.
        """
        ranges = np.array(laser_msg.ranges)
        valid_ranges = ranges[np.isfinite(ranges)]

        quality_metrics = {
            'valid_points_ratio': len(valid_ranges) / len(ranges),
            'min_range': np.min(valid_ranges) if len(valid_ranges) > 0 else float('inf'),
            'max_range': np.max(valid_ranges) if len(valid_ranges) > 0 else 0,
            'density': len(valid_ranges) / (laser_msg.angle_max - laser_msg.angle_min),
            'is_usable': (len(valid_ranges) / len(ranges)) > 0.5  # At least 50% valid points
        }

        return quality_metrics

    def assess_imu_quality(self, imu_msg, recent_imu_data):
        """
        Assess the quality of IMU data based on consistency.
        """
        if len(recent_imu_data) < 5:
            return {'is_usable': True, 'variance': 0.0}  # Not enough data to assess

        # Calculate variance of recent readings
        orientations = np.array([
            [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w]
            for msg in recent_imu_data
        ])

        variance = np.var(orientations, axis=0).mean()

        quality_metrics = {
            'variance': variance,
            'is_usable': variance < self.quality_thresholds['imu_variance']
        }

        return quality_metrics
```

## Best Practices for Sensor Data Reception

### 1. Efficient Data Handling
For high-frequency sensors, consider using appropriate queue sizes and QoS settings:

```python
# For high-frequency sensors like cameras
self.image_subscriber = self.create_subscription(
    Image,
    '/camera/image_raw',
    self.image_callback,
    1,  # Small queue to avoid old images
    # Use appropriate QoS for images
    qos_profile=rclpy.qos.QoSProfile(
        depth=1,
        reliability=rclpy.qos.ReliabilityPolicy.BEST_EFFORT,
        durability=rclpy.qos.DurabilityPolicy.VOLATILE
    )
)
```

### 2. Data Synchronization
For applications requiring synchronized sensor data:

```python
from rclpy.qos import QoSProfile, ReliabilityPolicy
from message_filters import ApproximateTimeSynchronizer, Subscriber

class SynchronizedSensorProcessor(Node):
    def __init__(self):
        super().__init__('sync_sensor_processor')

        # Create subscribers with specific QoS
        qos = QoSProfile(depth=10, reliability=ReliabilityPolicy.BEST_EFFORT)

        self.image_sub = Subscriber(self, Image, '/camera/image_raw', qos_profile=qos)
        self.laser_sub = Subscriber(self, LaserScan, '/scan', qos_profile=qos)

        # Synchronize messages (approximate time synchronization)
        self.sync = ApproximateTimeSynchronizer(
            [self.image_sub, self.laser_sub],
            queue_size=10,
            slop=0.1  # 100ms tolerance
        )
        self.sync.registerCallback(self.sync_callback)

    def sync_callback(self, image_msg, laser_msg):
        """
        Process synchronized image and laser data.
        """
        # Both messages are approximately time-synchronized
        self.process_fused_data(image_msg, laser_msg)
```

### 3. Memory Management
For sensor data processing:

```python
def __init__(self):
    super().__init__('ai_sensor_subscriber')

    # Use circular buffers for sensor history
    self.max_history = 100
    self.image_history = deque(maxlen=self.max_history)
    self.laser_history = deque(maxlen=self.max_history)

def image_callback(self, msg):
    """
    Store image in history, removing oldest if necessary.
    """
    # Convert and store image
    cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
    self.image_history.append(cv_image)

    # Process only if we have enough history
    if len(self.image_history) >= 10:
        self.analyze_image_trend()
```

## Summary

Python AI agents can receive sensor data from ROS 2 using three main patterns:

1. **Topic-based subscription**: Best for continuous sensor streams requiring real-time processing
2. **Service-based requests**: Best for on-demand sensor data retrieval with immediate response
3. **Action-based retrieval**: Best for complex sensor tasks with ongoing feedback

When implementing sensor data reception, consider data quality assessment, efficient processing, synchronization requirements, and appropriate QoS settings. The choice of communication pattern should match the specific requirements of the AI processing task.

### Cross-References to Related Sections

For deeper understanding of related concepts, see:
- [Introduction to Python Agent to ROS 2 Controller Bridge](./intro) - Overview of AI-ROS integration
- [How Python AI Agents Send Commands](./ai-ros-integration) - Sending commands to controllers
- [ROS 2 Architecture Understanding](../chapter-1/intro) - Communication patterns foundation
- [Python ROS 2 Node Creation](../chapter-2/intro) - Node implementation basics
- [Creating a Python Subscriber Node](../chapter-2/subscriber-node) - Detailed subscriber concepts